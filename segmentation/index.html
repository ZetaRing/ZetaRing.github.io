<!DOCTYPE html>
<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Zheng, Zishuo</title>
<link rel="preconnect" href="https://fonts.gstatic.com/">
<link rel="preload" href="../css_files/css" as="style" type="text/css" crossorigin="">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="theme-color" content="#157878">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<link rel="stylesheet" href="../css_files/style.css">
</head>

<body>
  <header class="page-header" role="banner">
    <h1 class="project-name">Semantic Segmentation</h1>    
  </header>

<main id="content" class="main-content" role="main">
<h2>Demo on Teams</h2>
<video width="640" height="480" controls>
<source src="./video.mp4" type="video/mp4">
</video>

<h2>Target & Obstacle</h2>
<p>Teams and Skype teams in Redmond have developed a neural network for semantic segmentation, which can segment a person out of the background in real time. This model is designed for running on personal computers and mobile phones. We need to improve the performance of this model.</p>
<p>1. Model size and running time are strictly restrained. We need to increase IoU under limited computational resources.<br></p>
<p>2. Original model is well-trained and finely modified. We do not have enough time and workforce to finetune a model. Consequently, our refinement method needs to be efficient enough to provide IoU improvement even without carefully selected parameters.<br></p>
<p>3. Original model is well-designed and highly compressed. We do not have much redundancy.</p>

<h2>Knowledge Distillation</h2>
<p>In Knowledge distillation (KD) (Hinton et al., NISP, 2014), the teacher is MobileNetV2 (Sandler et al., CVPR, 2018) with a width multiplier of 2.0, and the student is MobileNetV2 with a width multiplier of 0.5.</p>
<img src="./kd.png">
<p>The results are displayed below. X-axis: iterations. Y-axis: IoU on the test set.</p>
<img src="./kd_result.png">

<h2>Cross Knowledge Distillation</h2>
<p>We proposed cross-knowledge distillation (cross KD) inspired by KD. In cross KD, the student network is compelled to learn how to generate and decode the same feature extracted by the teacher network.</p>
<img src="./cross_kd.png">
<p>We found that cross KD can accelerate the speed of convergence significantly. However, the improvement of IoU is subtle under this scenario.</p>

<h2>Adaptive Scale</h2>
<p>Inspired by the squeeze and excitation network (Hu et al., CVPR, 2018), we proposed the adaptive scale.</p>
<h4> - Squeeze & Excitation (Hu et al., CVPR, 2018)</h4>
<p>Local Scale: extract global feature and multiply to itself.</p>
<img src="./local_scale.png">
<h4>- Fixed Scale</h4>
<p>Scale parameters are fixed to reduce the computational cost.</p>
<img src="./fixed_scale.png" height="300" >
<h4>- Global Adaptive Scale</h4>
<img src="./adaptive_scale.png">
<p>To balance performance and cost, we apply the global adaptive scale on the decoder and the fixed scale on the encoder.</p>
<h4>- Result</h4>
<p>Our method outperformed the raw network with little cost and no fine-tuning.</p>
<p>Input image size: 90x160x3</p>
<img src="./scale_result.png">
<p>(We used a new dataset here because more labeled data were available.)</p>

<h2>Pruning</h2>
<p>On the red circle, we will prune the elements in the weight matrix. No pruning will occur for the green circle, and the network can "recover" during training.</p>
<p>This training schema is in accord with the "Loss of Plasticity" theory proposed by Richard Sutton in <a href="https://www.youtube.com/watch?v=p_zknyfV9fY" target="_blank" style="color: blue;"><u>a talk in 2022</u></a>. However, we have applied it to practice in 2019.</p>
<img src="./lr.png">
<p>This network was trained on 4 x Tesla P40 for 117 hours.</p>
<p>Weight sparsity: 0% -> ~40%</p>
<img src="./sparsity.png">
<p>IoU on image: 95.48% -> 95.32%</p>

<footer class="site-footer">
<span class="site-footer-credits">This page was built based on the <a href="https://pages.github.com/" target="_blank" style="color: gray;"><u>GitHub Pages</u></a> project.</span>
</footer>
</main>
</body></html>